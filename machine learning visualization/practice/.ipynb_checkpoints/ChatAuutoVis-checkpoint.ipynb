{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "d0EiRNUYHcMF"
   },
   "source": [
    "# ChatGPT and AutoVis\n",
    "\n",
    "In this lab, you will learn to use OpenAI's ChatGPT API and to develope an automatical visualization system with ChatGPT.\n",
    "\n",
    "The visualization system should take input a user instruction and generate a corrsponding visualization. The user instruction examples are \"Show the distribution of [column]\" and \"Show the trend of the [column]\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pyvapcSsIPn7"
   },
   "source": [
    "## Preparation\n",
    "Install the openai package and set the OpenAI's API key. You need to register your key for the future usage if you are interested in.\n",
    "\n",
    "We are using Altair visualization package, which only requires the input of the JSON file. You do not need to know the package because ChatGPT knows it better than you! \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "paCHcP62HfJn",
    "outputId": "cc5d3c69-1720-4351-cd29-4e335e953f7f"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (0.27.0)\n",
      "Collecting altair\n",
      "  Downloading altair-4.2.2-py3-none-any.whl (813 kB)\n",
      "\u001b[K     |████████████████████████████████| 813 kB 4.7 MB/s eta 0:00:01\n",
      "\u001b[?25hCollecting vega_datasets\n",
      "  Downloading vega_datasets-0.9.0-py3-none-any.whl (210 kB)\n",
      "\u001b[K     |████████████████████████████████| 210 kB 7.2 MB/s eta 0:00:01\n",
      "\u001b[?25hRequirement already satisfied: requests>=2.20 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from openai) (2.27.1)\n",
      "Requirement already satisfied: tqdm in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from openai) (4.64.0)\n",
      "Requirement already satisfied: aiohttp in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from openai) (3.8.1)\n",
      "Requirement already satisfied: entrypoints in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from altair) (0.4)\n",
      "Requirement already satisfied: jsonschema>=3.0 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from altair) (4.4.0)\n",
      "Requirement already satisfied: numpy in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from altair) (1.21.5)\n",
      "Requirement already satisfied: toolz in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from altair) (0.11.2)\n",
      "Requirement already satisfied: jinja2 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from altair) (2.11.3)\n",
      "Requirement already satisfied: pandas>=0.18 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from altair) (1.4.2)\n",
      "Requirement already satisfied: pyrsistent!=0.17.0,!=0.17.1,!=0.17.2,>=0.14.0 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=3.0->altair) (0.18.0)\n",
      "Requirement already satisfied: attrs>=17.4.0 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from jsonschema>=3.0->altair) (21.4.0)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.18->altair) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from pandas>=0.18->altair) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from python-dateutil>=2.8.1->pandas>=0.18->altair) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2021.10.8)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (2.0.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (1.26.9)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from requests>=2.20->openai) (3.3)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (4.0.1)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.6.3)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (1.2.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from aiohttp->openai) (5.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.5 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from async-timeout<5.0,>=4.0.0a3->aiohttp->openai) (4.1.1)\n",
      "Requirement already satisfied: MarkupSafe>=0.23 in /Users/korahughes/opt/anaconda3/lib/python3.9/site-packages (from jinja2->altair) (2.0.1)\n",
      "Installing collected packages: vega-datasets, altair\n",
      "Successfully installed altair-4.2.2 vega-datasets-0.9.0\n"
     ]
    }
   ],
   "source": [
    "!pip install openai altair vega_datasets\n",
    "import openai\n",
    "import altair as alt\n",
    "import json\n",
    "from vega_datasets import data\n",
    "\n",
    "cars = data.cars()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "wAZoADpoHh7r"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<div id=\"altair-viz-05cd363fa0734e0688e5705a6a42d582\"></div>\n",
       "<script type=\"text/javascript\">\n",
       "  var VEGA_DEBUG = (typeof VEGA_DEBUG == \"undefined\") ? {} : VEGA_DEBUG;\n",
       "  (function(spec, embedOpt){\n",
       "    let outputDiv = document.currentScript.previousElementSibling;\n",
       "    if (outputDiv.id !== \"altair-viz-05cd363fa0734e0688e5705a6a42d582\") {\n",
       "      outputDiv = document.getElementById(\"altair-viz-05cd363fa0734e0688e5705a6a42d582\");\n",
       "    }\n",
       "    const paths = {\n",
       "      \"vega\": \"https://cdn.jsdelivr.net/npm//vega@5?noext\",\n",
       "      \"vega-lib\": \"https://cdn.jsdelivr.net/npm//vega-lib?noext\",\n",
       "      \"vega-lite\": \"https://cdn.jsdelivr.net/npm//vega-lite@4.17.0?noext\",\n",
       "      \"vega-embed\": \"https://cdn.jsdelivr.net/npm//vega-embed@6?noext\",\n",
       "    };\n",
       "\n",
       "    function maybeLoadScript(lib, version) {\n",
       "      var key = `${lib.replace(\"-\", \"\")}_version`;\n",
       "      return (VEGA_DEBUG[key] == version) ?\n",
       "        Promise.resolve(paths[lib]) :\n",
       "        new Promise(function(resolve, reject) {\n",
       "          var s = document.createElement('script');\n",
       "          document.getElementsByTagName(\"head\")[0].appendChild(s);\n",
       "          s.async = true;\n",
       "          s.onload = () => {\n",
       "            VEGA_DEBUG[key] = version;\n",
       "            return resolve(paths[lib]);\n",
       "          };\n",
       "          s.onerror = () => reject(`Error loading script: ${paths[lib]}`);\n",
       "          s.src = paths[lib];\n",
       "        });\n",
       "    }\n",
       "\n",
       "    function showError(err) {\n",
       "      outputDiv.innerHTML = `<div class=\"error\" style=\"color:red;\">${err}</div>`;\n",
       "      throw err;\n",
       "    }\n",
       "\n",
       "    function displayChart(vegaEmbed) {\n",
       "      vegaEmbed(outputDiv, spec, embedOpt)\n",
       "        .catch(err => showError(`Javascript Error: ${err.message}<br>This usually means there's a typo in your chart specification. See the javascript console for the full traceback.`));\n",
       "    }\n",
       "\n",
       "    if(typeof define === \"function\" && define.amd) {\n",
       "      requirejs.config({paths});\n",
       "      require([\"vega-embed\"], displayChart, err => showError(`Error loading script: ${err.message}`));\n",
       "    } else {\n",
       "      maybeLoadScript(\"vega\", \"5\")\n",
       "        .then(() => maybeLoadScript(\"vega-lite\", \"4.17.0\"))\n",
       "        .then(() => maybeLoadScript(\"vega-embed\", \"6\"))\n",
       "        .catch(showError)\n",
       "        .then(() => displayChart(vegaEmbed));\n",
       "    }\n",
       "  })({\"config\": {\"view\": {\"continuousWidth\": 300, \"continuousHeight\": 300}}, \"data\": {\"url\": \"https://vega.github.io/vega-datasets/data/cars.json\"}, \"mark\": {\"type\": \"point\"}, \"encoding\": {\"color\": {\"field\": \"Origin\", \"type\": \"nominal\"}, \"x\": {\"field\": \"Horsepower\", \"type\": \"quantitative\"}, \"y\": {\"field\": \"Miles_per_Gallon\", \"type\": \"quantitative\"}}, \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\"}, {\"mode\": \"vega-lite\"});\n",
       "</script>"
      ],
      "text/plain": [
       "alt.Chart(...)"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Example of using Altair: input a json and generate the visualization\n",
    "\n",
    "chart_json_str = \"\"\"{\n",
    "  \"$schema\": \"https://vega.github.io/schema/vega-lite/v5.json\",\n",
    "  \"config\": {\n",
    "    \"view\": {\n",
    "      \"continuousHeight\": 300,\n",
    "      \"continuousWidth\": 300\n",
    "    }\n",
    "  },\n",
    "  \"data\": {\n",
    "    \"url\": \"https://vega.github.io/vega-datasets/data/cars.json\"\n",
    "  },\n",
    "  \"encoding\": {\n",
    "    \"color\": {\n",
    "      \"field\": \"Origin\",\n",
    "      \"type\": \"nominal\"\n",
    "    },\n",
    "    \"x\": {\n",
    "      \"field\": \"Horsepower\",\n",
    "      \"type\": \"quantitative\"\n",
    "    },\n",
    "    \"y\": {\n",
    "      \"field\": \"Miles_per_Gallon\",\n",
    "      \"type\": \"quantitative\"\n",
    "    }\n",
    "  },\n",
    "  \"mark\": {\"type\": \"point\"}\n",
    "}\"\"\"\n",
    "chart_json = json.loads(chart_json_str)\n",
    "new_chart = alt.Chart.from_json(chart_json_str)\n",
    "new_chart.display()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "StupskHQIzEX"
   },
   "source": [
    "## ChatGPT API\n",
    "To use ChatGPT API, you need to input the API key and the message history. The message history looks in the way:\n",
    "```\n",
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Who won the battle of Waterloo?\"},\n",
    "        ]\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 69
    },
    "id": "wlkb4qFdHjnQ",
    "outputId": "5d0ab0d4-7ca1-4825-b6bf-a81f6f8b3729"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'The Battle of Waterloo was won by the forces of the Seventh Coalition, comprised of the British army led by the Duke of Wellington, along with Prussian forces under the command of Field Marshal Gebhard Leberecht von Blücher. The battle took place on June 18, 1815, and resulted in the defeat of French Emperor Napoleon Bonaparte.'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def run_gpt4(messages):\n",
    "  OPENAI_API_KEY = \"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\"\n",
    "  openai.api_key = OPENAI_API_KEY\n",
    "  response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4\",\n",
    "        messages=messages\n",
    "    )\n",
    "  return response[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "messages = [\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": \"Who won the battle of Waterloo?\"},\n",
    "        ]\n",
    "run_gpt4(messages)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DHs9E75hJAzY"
   },
   "source": [
    "## ChatGPT\n",
    "Write a simple command-line application for communicating with ChatGPT. You just need to use Python the `input` function to get the user input and send it to ChatGPT. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "FH1RILeYHnen",
    "outputId": "b350dab4-c344-463a-8c11-32fc3ad6cdce"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "user: do you have any developer tools?\n",
      "chat_gpt:  As an AI language model, I do not have built-in developer tools. However, I can offer help, guidance, and suggestions for using various external developer tools. Some popular developer tools include:\n",
      "\n",
      "1. Integrated Development Environments (IDEs) like Visual Studio, JetBrains, or Eclipse.\n",
      "2. Code editors, such as Visual Studio Code, Atom, or Sublime Text.\n",
      "3. Version control systems like Git, Mercurial, or SVN.\n",
      "4. Debugging tools like Chrome DevTools or Firebug.\n",
      "5. Performance profiling tools like Lighthouse, WebPageTest, or SpeedCurve.\n",
      "6. Task runners and build tools, such as npm scripts, Grunt, or Gulp.\n",
      "7. Test frameworks, like Jest, Mocha, Jasmine, or Selenium.\n",
      "8. Continuous integration and deployment tools, like Jenkins, Travis CI, or CircleCI.\n",
      "\n",
      "Let me know which tool or aspect of development you need assistance with, and I will gladly try to help. \n",
      "\n",
      "user: finish this sentence: \"the mitochondria is...\"\n",
      "chat_gpt:  the powerhouse of the cell, responsible for generating most of the cell's energy in the form of adenosine triphosphate (ATP). \n",
      "\n",
      "user: finish this sentence: \"Zǎo shang hǎo zhōng guó! Xiàn zài wǒ yǒu\"\n",
      "chat_gpt:  \"Zǎo shang hǎo zhōng guó! Xiàn zài wǒ yǒu yī gè wèn tí xiǎng wèn nǐ.\" (早上好中国！现在我有一个问题想问你。)\n",
      "\n",
      "In English, this sentence means: \"Good morning, China! Now, I have a question to ask you.\" \n",
      "\n",
      "user: what is your favorite meme?\n",
      "chat_gpt:  As an AI language model, I don't have personal preferences or feelings, so I don't have a favorite meme. However, I can understand and discuss various memes that are popular or that you find interesting. Feel free to mention a specific meme or ask me any questions related to memes. \n",
      "\n",
      "user: what is the most recent meme?\n",
      "chat_gpt:  As an AI, I don’t have real-time updates on the most recent memes. Memes constantly emerge and evolve through social media platforms and various websites. To find the most recent memes, you may want to explore websites like Reddit, Twitter, or Instagram and look for meme-related posts or hashtags. Keep in mind that since memes are continuously created and trends change rapidly, it can be difficult to pinpoint a single \"most recent\" meme. \n",
      "\n",
      "user:  exit\n",
      "chat_gpt:  If you need any further assistance or have more questions, don't hesitate to ask. Have a great day! \n",
      "\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "Interrupted by user",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 17>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mchat_gpt: \u001b[39m\u001b[38;5;124m\"\u001b[39m, response, \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     15\u001b[0m         message_hist\u001b[38;5;241m.\u001b[39mappend({\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msystem\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: response})\n\u001b[0;32m---> 17\u001b[0m \u001b[43mchat_with_gpt\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Input \u001b[0;32mIn [14]\u001b[0m, in \u001b[0;36mchat_with_gpt\u001b[0;34m(previous_hist)\u001b[0m\n\u001b[1;32m      6\u001b[0m     message_hist \u001b[38;5;241m=\u001b[39m previous_hist\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m----> 9\u001b[0m     user_message \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43minput\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43muser: \u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     10\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m user_message \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mexit\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[1;32m     11\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m message_hist\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py:1075\u001b[0m, in \u001b[0;36mKernel.raw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m   1071\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_allow_stdin:\n\u001b[1;32m   1072\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m StdinNotImplementedError(\n\u001b[1;32m   1073\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mraw_input was called, but this frontend does not support input requests.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1074\u001b[0m     )\n\u001b[0;32m-> 1075\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_input_request\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   1076\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mstr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mprompt\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1077\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_parent_ident\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1078\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_parent\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mshell\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   1079\u001b[0m \u001b[43m    \u001b[49m\u001b[43mpassword\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m   1080\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/opt/anaconda3/lib/python3.9/site-packages/ipykernel/kernelbase.py:1120\u001b[0m, in \u001b[0;36mKernel._input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m   1117\u001b[0m             \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[1;32m   1118\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m:\n\u001b[1;32m   1119\u001b[0m     \u001b[38;5;66;03m# re-raise KeyboardInterrupt, to truncate traceback\u001b[39;00m\n\u001b[0;32m-> 1120\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyboardInterrupt\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInterrupted by user\u001b[39m\u001b[38;5;124m\"\u001b[39m) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlog\u001b[38;5;241m.\u001b[39mwarning(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mInvalid Message:\u001b[39m\u001b[38;5;124m\"\u001b[39m, exc_info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def chat_with_gpt(previous_hist=[]):\n",
    "    message_hist = []  # init\n",
    "    if len(previous_hist) == 0:\n",
    "        message_hist = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"}]\n",
    "    else:\n",
    "        message_hist = previous_hist\n",
    "        \n",
    "    while True:\n",
    "        user_message = input(\"user: \")\n",
    "        if user_message == \"exit\":\n",
    "            return message_hist\n",
    "        else if user_message[0] == \"\\\\\":  # add context to chat gpt\n",
    "            message_hist.append({\"role\": \"system\", \"content\": user_message})\n",
    "            print(\"chat_gpt: \", user_message, '\\n')\n",
    "        else:\n",
    "            message_hist.append({\"role\": \"user\", \"content\": user_message})\n",
    "            response = run_gpt4(message_hist)\n",
    "            print(\"chat_gpt: \", response, '\\n')\n",
    "            message_hist.append({\"role\": \"system\", \"content\": response})\n",
    "\n",
    "print(\"\\n\\n\", chat_with_gpt())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kanDlJVeJU7o"
   },
   "source": [
    "## Automatical Visualization\n",
    "Write a simple command-line application for generating the chart with ChatGPT. You need to collect the user input by using the same `input` function. The instruction should be sent to ChatGPT. We only deal with the cars dataset so you do not need to upload the dataset.\n",
    "\n",
    "Function `visualization_with_json` takes input of a text instruction and generate a chart. You do not need to change that."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 341
    },
    "id": "gabew-gXHqy3",
    "outputId": "54fc7d3e-a261-4532-9137-6bc4ce151e3c"
   },
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-0ec12e42b166>\u001b[0m in \u001b[0;36m<cell line: 23>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m   \u001b[0mvisualization_with_json\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mvisualization_with_gpt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-0ec12e42b166>\u001b[0m in \u001b[0;36mvisualization_with_gpt\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m       \u001b[0;34m{\u001b[0m\u001b[0;34m\"role\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;34m\"system\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"content\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mprompt_instruction\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m   ]\n\u001b[0;32m---> 15\u001b[0;31m   \u001b[0muser_instruction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"user:\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m   result = run_gpt4([\n\u001b[1;32m     17\u001b[0m       \u001b[0mmessages\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36mraw_input\u001b[0;34m(self, prompt)\u001b[0m\n\u001b[1;32m    849\u001b[0m                 \u001b[0;34m\"raw_input was called, but this frontend does not support input requests.\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    850\u001b[0m             )\n\u001b[0;32m--> 851\u001b[0;31m         return self._input_request(str(prompt),\n\u001b[0m\u001b[1;32m    852\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_ident\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    853\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_parent_header\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.9/dist-packages/ipykernel/kernelbase.py\u001b[0m in \u001b[0;36m_input_request\u001b[0;34m(self, prompt, ident, parent, password)\u001b[0m\n\u001b[1;32m    893\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    894\u001b[0m                 \u001b[0;31m# re-raise KeyboardInterrupt, to truncate traceback\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 895\u001b[0;31m                 \u001b[0;32mraise\u001b[0m \u001b[0mKeyboardInterrupt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Interrupted by user\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    896\u001b[0m             \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    897\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwarning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"Invalid Message:\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexc_info\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: Interrupted by user"
     ]
    }
   ],
   "source": [
    "def visualization_with_json(chart_json_str):\n",
    "  new_chart = alt.Chart.from_json(chart_json_str)\n",
    "  new_chart.display()\n",
    "\n",
    "def visualization_with_gpt():\n",
    "  prompt_instruction = \"\"\"\n",
    "    You are data visualization bot, which transforms the user's natural language instruction into the data visualization formatted by altair json.\n",
    "    The all visualizations are based on the cars dataset in vega_datasets.\n",
    "    The user'instruction example is showing the distribution of the Accelaration. For that instruction, you need to create a histogram for the column of Acceleration.\n",
    "    Remember you only need to return JSON.\n",
    "  \"\"\"\n",
    "\n",
    "  user_instruction = input(\"user:\")\n",
    "  result = run_gpt4([\n",
    "      \n",
    "  ])\n",
    "  print(result)\n",
    "  visualization_with_json(result)\n",
    "\n",
    "visualization_with_gpt()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q6o0V-ZhJv3Q"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "ChatAuutoVis.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
